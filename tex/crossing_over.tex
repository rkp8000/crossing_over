\documentclass{article}


\usepackage{arxiv}
\usepackage{amsmath}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}

\title{A hyperdimensional crossover code for sequences and k-ary trees}

\author{
  Rich ~Pang\thanks{https://rkp.science} \\
  Computational Neuroscience Center\\
  Department of Physiology and Biophysics\\
  University of Washington\\
  Seattle, WA 98195-7290 \\
  \texttt{rpang at uw dot edu} \\
}

\begin{document}
\maketitle

\begin{abstract}
Hyperdimensional computing (HDC) is a distributed computing framework where information is represented by simple functions of high-dimensional vectors with i.i.d. random components. This confers several advantages, such as well separated, fixed-dimensional codes for composite objects like sequences, and robustness to noise. A key challenge is developing HDC codes that optimally preserve similarities among the objects they represent while simultaneously keeping their representations easily distinguishable. HDC codes for sequences, for instance, typically differ strongly from the codes for their elements (e.g. the code for AB is equidistant from those for A, B, and C) in order to retain order information. I present a novel HDC method that encodes sequences not via convolution, permutation, or matrix multiplication, which so far have driven the field, but via a simple operation analogous to chromosomal crossover; the resulting sequence vectors retain both recoverable order information and high similarity to the codes for their elements, thus better preserving desirable similarities. Crossover naturally generalizes to k-ary trees, suggesting general utility in the representation of arbitrary objects for general cognition. I derive several mathematical properties of crossover and discuss its relevance to artificial intelligence and computational neuroscience.
\end{abstract}


% keywords can be removed
\keywords{Hyperdimensional computing \and Vector symbolic architectures \and Sequence encoding \and k-ary tree encoding \and Computational neuroscience}


\section{Introduction}
\lipsum[2]
\lipsum[3]


\section{Methods}

Consider a system $X$ of $N$ components $x_1, ..., x_N$, each of which can take any of $Z$ states. That is, $x_j \in G$ where $|G| = Z$. Label the states of $G$ as $1,...,Z$ for conciseness, and for clarity call these component states "c-states", to distinguish them from the overall system state.

Let the distance $d(X^A, X^B)$ between two states be their Hamming distance, scaled to range between 0 and 1, and their similarity $s(X^A, X^B) \equiv 1 - d(X^A, X^B)$. If $x_j$ are sampled uniformly and i.i.d from $G$, then $s(X^A, X^B) \sim \frac{1}{N} \textrm{Bi}(N, q)$, where $\textrm{Bi}(N, q)$ is a binomial distribution with success probability $q = 1/Z$. The mean similarity of two such system states is thus $E[s(X^A, X^B)] = 1/Z$.

We wish to establish a decodable mapping from arbitrary objects of cognition to system states, such that objects that are intuitively similar to us map to system states with high similarities. We shall start by considering sequences of symbols then generalize to arbitrary finite nested sets (or equivalently k-ary trees) with symbols as elements (leaves).

\subsection{Algorithm}

Given a dictionary $D$ of $M$ symbols $s^1, ..., s^M$, we will first sample a "C-basis" (crossover basis) $B$ and an auxiliary mask function $\mu$ from $P(B)$ and $P(\mu)$, respectively. Using $B$ and $\mu$, we will then deterministically build codes $\{X^Y\}$ of arbitrary sequences $\{Y = (y_1, ..., y_L)\}$, where $y_t \in D$.

\textbf{Setup}

1. Sample a C-basis $B \equiv (X^1, ..., X^M, X^*)$, where each $X^i$ (encoding symbol $s^i$) has components $x^i_j$ sampled uniformly and i.i.d. from $G$, and where $X^*$ is "start" code, also comprised of uniform, i.i.d. components from $\Omega$.

2. Sample a "mask function" $\mu \sim P(\mu)$, where $\mu:(X^A, X^B) \rightarrow [0, 1]^N$ and is represented by a matrix with $Z^{2N}$ rows (indexed by $(X^A, X^B)$) and $N$ columns (indexed by $j$), with i.i.d. real-valued elements from $\textrm{Uniform}(0, 1)$.

In practice $\mu$ need not be sampled explicitly, which would take astronomical time; it suffices to use, say, a hash function like MD5 or SHA-1 to map $(X^A, X^B)$ to an integer, seed an RNG with this integer, then sample $N$ times from $\textrm{Uniform}(0, 1)$ using the seeded RNG. $\mu$ as defined above simply allows cleaner mathematical rigor.

\textbf{Encoding}

...

\section{Key properties}

\subsection{Independence of component states}
\label{sec:prop-ind-c-states}

\textbf{Result:}

$$P(X^Y|Y) = \prod\limits_{j=1}^N P(x^Y_j|Y) = P(x^Y_j|Y)^N$$

\textbf{Proof:}

\subsection{Equivalence of c-states}
\label{sec:prop-equiv-c-states}

\textbf{Result:}

$$P(x^Y_j = g|Y) = P(x^Y_j = g'|Y) = \frac{1}{Z}, \forall g, g' \in G.$$

\textbf{Proof:}

We will show that $P(x^Y_j = g|Y) = P(x^Y_j = g'|Y)$ for all $r \neq r'$. This implies $P(x^Y_j = g|Y) = 1/Z$ (since $g \in G$ and $|G| = Z$), which is not a function of Y.

As $x^Y_j$ can only depend on $\tilde{x} \equiv (x^1_j, ..., x^M_j, x^*_j)$, we can expand

$$P(x^Y_j = r|Y) = \sum\limits_{\tilde{x}} P(x^Y_j = r|\tilde{x}, Y)P(\tilde{x}|Y) = \frac{1}{Z^{M+1}}\sum\limits_{\tilde{x}} P(x^Y_j = r|\tilde{x}, Y).$$

The sum comprises terms where either $g \in \tilde{x}$ or $g \notin \tilde{x}$. In the latter terms, $P(x^Y_j = g|\tilde{x}, Y) = 0$. Thus we only need to worry about the former terms where $g \in \tilde{x}$.

The trick is now to realize that each of these remaining terms has exactly one partner in the equivalent expansion of $P(x^Y_j = g'|Y)$, with the $g$ and $g'$ c-states swapped. Thus, the expansion for both $P(x^Y_j = g|Y)$ and $P(x^Y_j = g'|Y)$ have exactly the same set of terms, so they must be equal. Therefore, $P(x^Y_j = g|Y) = 1/Z$. 

Thus, $P(X^Y|Y) = P(X^Y)$ so the distribution of sequence codes is independent of sequence length, or any sequence statistics for that matter, when marginalized over all possible crossover bases $B$. This distinguishes crossover from, say, binary encodings that grow sparser or denser with sequence length.

\subsection{Invariance of code statistics to sequence statistics}

\textbf{Result:}

$$P(X^Y|Y) \textrm{ does not depend on any statistic (e.g. length, symbol repetitions, etc) of } Y.$$

\textbf{Proof:}

This follows from \ref{sec:prop-ind-c-states} and \ref{sec:prop-equiv-c-states}.

\section{Similarity metrics}

It follows from property \ref{sec:prop-ind-c-states} that $s(X^Y, X^{Y'}) \sim \frac{1}{N}\textrm{Bi}(N, q)$, where $\textrm{Bi}(N, q)$ is a binomial distribution with success probability $q = P(x^Y_j = x^{Y'}_j)$. This because for a given code $X$ all components are independent, and code construction furthermore has no dependence on $j$. Thus, $\textrm{E}[s(X^Y, X^{Y'})] = q$ and $\textrm{Var}[s(X^Y, X^{Y'})] = q(1-q)/N$, and it suffices to only compute the probability that the $j$-th element (where $j$ is arbitrary) of $X^Y$ and $X^{Y'}$ is in the same c-state. For notational conciseness, we thus drop the $j$ and write $x \equiv x_j$.

Another useful definition is $P(x^Y \leftarrow X^i)$, the probability that in constructing $X^Y$, the component $x^Y$ was copied from symbol code $X^i$. If this is true, then $x^Y = x^i$. Note that the converse is not true, however: even if $x^Y$ was taken from a symbol code $X^k \neq X^i$, then $x^Y$ could still equal $x^i$, since there is a chance $x^k = x^i = x^Y$. We define $P(x^Y \not \leftarrow X^i) = 1 - P(x^Y \leftarrow X^i)$.

\subsection{Similarity between start code and sequence code}
\label{sec:sml-start-seq}

\textbf{Result}:

$$\textrm{E}[s(X^*, X^{Y'})] = P(x^* = x^Y) = \frac{1}{L+1} + \frac{1}{Z}\frac{L}{L+1}$$
which tends to $1/(L+1)$ for $Z \gg L$.

\textbf{Proof}:

We compute $P(x^* = x^Y)$ by expanding over all c-states $g \in G$:

$$P(x^* = x^Y) = P(x^* = g, x^Y = g) = \sum\limits_g P(x^* = g, x^Y = g|x^* = g)P(x^* = g)$$

$$ = \frac{1}{Z}\sum\limits_g P(x^* = g, x^Y = g|x^* = g) = \frac{1}{Z}\sum\limits_g P(x^Y = g|x^* = g).$$

Further, since after marginalizing over $B$ and $\mu$ the encoding can favor no $g$ in particular,

$$\frac{1}{Z}\sum\limits_g P(x^Y = g|x^* = g) = \frac{1}{Z}ZP(x^Y = g|x^* = g) = P(x^Y = g|x^* = g)$$

for any arbitrary $g$.

We now expand over the probability $P(x^Y \leftarrow X^*)$ that $x^Y$ in the sequence encoding came originally from $X^*$, which is 1 minus the probability $P(x^Y \not \leftarrow X^*)$ that $x^*$ was overwritten at some later point as the subsequent symbol codes were mixed in (even if it was overwritten by the same c-state $g$):

$$P(x^Y = g|x^* = g) = P(x^Y = g|x^Y \leftarrow X^*, x^* = g)P(x^Y \leftarrow X^*) + P(x^Y = g|x^Y \not \leftarrow X^*, x^* = g)P(x^Y \not \leftarrow X^*)$$

$$= P(x^Y \leftarrow X^*) + \frac{1}{Z}P(x^Y \not \leftarrow X^*)$$,

where the $1/Z$ comes from the fact that all the $X^i$ corresponding to $y_1, ..., y_L$ are independent from $x^*$, so no matter which $X^i$ the c-state of $x^Y$ came from, it has a $1/Z$ chance of being the same as $x^*$. $P(x^Y \leftarrow X^*)$ is the probability $x^*$ was never masked over in the encoding process:

$$P(x^Y \leftarrow X^*) = \frac{1}{2}\frac{2}{3}...\frac{L}{L+1} = \frac{1}{L+1}.$$

Thus

$$P(x^* = x^Y) = \frac{1}{L+1} + \frac{1}{Z}\left(1 - \frac{1}{L+1}\right) = \frac{1}{L+1} + \frac{1}{Z}\frac{L}{L+1}.$$

\subsection{Similarity between symbol codes and the sequence code}

\textbf{Result:}

Let $\mathbf{n} = (n^1, ..., n^M)$ be a vector counting how many times each symbol $s^i$ is contained in $Y$, where $\sum\limits_i n_i = L.$
If $s^i$ is contained $n^i$ times in $Y$ at locations $1 \leq t_1 <  ... < t_{n^i} \leq L$, then:

$$P(x^Y = x^i) = \frac{n^i(Z-1) + L + 1}{(L+1)Z}$$

which is independent of $t_1, ..., t_{n^i}$ and tends to $n^i/(L+1)$ for $Z \gg L.$ Thus, there will be large differences in similarities between $X^Y$ and $X^i$ as a function of how many times $s_i$ appeared in $Y$.

\textbf{Proof:}

$P(x^Y \neq x^i)$ is the probability $P(x^Y \not\leftarrow X_i)$ that $x^Y$ was either not taken from $X_i$ at any $t_k$ or was at some point overwritten, times $(Z-1)/Z$, the probability it was initially or at some point overwritten by something other than $x^i$, (since all symbol codes it could have been overwritten by are independent from $X^i$). In other words, 

$$P(x^Y \neq x^i) = P(x^Y \not\leftarrow X^i)\frac{Z-1}{Z} = \left(1 - P(x^Y \leftarrow X^i)\right)\frac{Z-1}{Z} $$.

Here $P(x^Y_j \leftarrow X_i)$ is the sum of the probabilities of all the ways $x^Y$ could have ended up being taken from $X^i$ without being overwritten later by the code for a different symbol $s^{i'} \neq s^i$. For instance, $x^Y$ could have been taken from $X^i$ at $t_k$ then left untouched for the remainder of the sequence construction; or it could have been taken from $X^i$ at both $t_k$ and $t_{k+1}$ then left untouched after $t_{k+1}$, and so forth.

We can work backwards by first considering all cases where $x^Y$ was taken from $X^i$ at $t_{n^i}$ then left untouched; then all cases where $x^Y$ was taken from $X^i$ at $t_{n^i-1}$ then left untouched, even at $t_{n^i}$; then all cases where $x^Y$ was taken from $X^i$ at $t_{n^i-2}$ then left untouched, even at $t_{n^i-1}$ and $t_{n^i}$, and so on. All these sets of cases are mutually exclusive and furthermore cover all the combinatorial ways in which $x^Y$ could have been taken from $X^i$, so at the end we can just add up the probabilities.

Conveniently, the sum of the probabilities over the first set of cases is just the probability $x^Y$ was taken from $X^i$ at $t_{n^i}$ and not overwritten later, since everything that happened before $t_{n^i}$ is irrelevant and marginalizes out. This probability is just 

$$\frac{1}{t_{n^i}+1}\frac{t_{n^i}+1}{t_{n^i}+2}...\frac{L}{L+1} = \frac{1}{L+1}$$

Similar logic applies for all $n^i$ sets of cases, so the total probability

$$P(x^Y \leftarrow X^i) = \frac{n^i}{L+1} \textrm{, and } P(x^Y \not\leftarrow X^i) = \frac{L+1-n^i}{L+1}$$

and therefore 

$$P(x^Y = x^i) = 1 - P(x^Y \neq X^i) = 1 - \frac{L-n^i+1}{L+1}\frac{Z-1}{Z} = \frac{n^i(Z-1) + L + 1}{(L+1)Z}.$$

\subsection{Similarity between codes for arbitrary sequences that share no starting subsequence}

Let $Y = (y_1, ..., y_L)$ and $Y' = (y'_1, .., y'_{L'})$ be two sequences. 

When $y_1 \neq y'_1$ (i.e. $Y$ and $Y'$ share no starting subsequence), then

$$P(x^Y = x^{Y'}) = \frac{1}{(L+1)(L'+1)}\left[1 + \frac{L + L' + LL'}{Z} + \frac{Z-1}{Z}\mathbf{n}^T\mathbf{n}'\right]$$

\textbf{Proof:}

We first consider sequences where $y_1 \neq y'_1$, since then all mask operations are independent in constructing $X^Y$ and $X^{Y'}$. In this case we compute $P(x^Y = x^{Y'})$ by enumerating all nonoverlapping cases where this can occur then adding up their probabilities. These are:

Case 1: $(x^Y \leftarrow X^*) and (x^{Y'} \leftarrow X^*)$, i.e. both $x$'s are taken from the start code i.e. not overwritten. These are independent, so

$$P(\textrm{Case 1}) = P(x^Y \leftarrow X^*)P(x^{Y'} \leftarrow X^*) = \frac{1}{L+1}\frac{1}{L'+1}$$

Case 2: $(x^Y \leftarrow X^*) and (x^{Y'} \not \leftarrow X^*) and (x^{Y'} = x^*)$, i.e. $x^Y$ is taken from $X^*$, whereas $x^{Y'}$ is overwritten by a state that just happens to equal $x^*$. The first term is independent from the second two, such that

$$P(\textrm{Case 2}) = P(x^Y \leftarrow X^*)P(x^{Y'} \not \leftarrow X^*, x^{Y'} = x^*)$$

$$ = \frac{1}{L+1}P(x^{Y'} \not \leftarrow X^*)P(x^{Y'} = x^*|x^{Y'} \not \leftarrow X^*) = \frac{1}{L+1}\frac{L'}{L'+1}\frac{1}{Z}.$$

Case 3: $(x^{Y'} \leftarrow X^*) and (x^Y \not \leftarrow X^*) and (x^Y = x^*)$. This is symmetric to the previous case, so 

$$P(\textrm{Case 3}) = \frac{1}{L'+1}\frac{L}{L+1}\frac{1}{Z}$$

Case 4: $(x^Y \not \leftarrow X^*) and (x^{Y'} \not \leftarrow X^*) and (x^Y = x^{Y'})$ i.e. both $x$'s are overwritten but are overwritten by the same state. The first two events are independent, so

$$P(\textrm{Case 4}) = \frac{L}{L+1}\frac{L'}{L'+1}P(x^Y = x^{Y'} | x^Y, x^{Y'} \not \leftarrow X^*)$$

The final term depends on the structure of the two sequences (e.g. it will be high for $Y =$ "abbb" and $Y' =$ "cbbb" but low for $Y =$ "abcd" and $Y' =$ "efgh"). Denoting $X^i$ and $X^{i'}$ as the two symbol codes $x^Y$ and $x^{Y'}$ were taken from during the code construction, respectively, we can expand $P(x^Y = x^{Y'} | x^Y, x^{Y'} \not \leftarrow X^*) =$

$$P(X^i = X^{i'}| x^Y, x^{Y'} \not \leftarrow X^*)P(x^Y = x^{Y'}|X^i = X^{i'}; x^Y, x^{Y'} \not \leftarrow X^*)$$

$$+ P(X^i \neq X^{i'} | x^Y, x^{Y'} \not \leftarrow X^*)P(x^Y = x^{Y'}|X^i \neq X^{i'}; x^Y, x^{Y'} \not \leftarrow X^*)$$

$$ = P(X^i = X^{i'}| x^Y, x^{Y'} \not \leftarrow X^*) + P(X^i \neq X^{i'}| x^Y, x^{Y'} \not \leftarrow X^*)\frac{1}{Z}.$$

To compute $P(X^i = X^{i'}| x^Y, x^{Y'} \not \leftarrow X^*)$, the probability that $x^Y$ and $x^{Y'}$ were taken from the same symbol code, we first note that the probability $x^Y$ was taken from $X^i$ is independent from the probability $x^{Y'}$ was taken from $X^{i'}$, i.e.

$$p(x^Y \leftarrow X^i, x^{Y'} \leftarrow X^{i'}| x^Y, x^{Y'} \not \leftarrow X^*)$$

$$ = p(x^Y \leftarrow X^i| x^Y, x^{Y'} \not \leftarrow X^*)p(x^{Y'} \leftarrow X^{i'}| x^Y, x^{Y'} \not \leftarrow X^*)= \frac{n^i}{L}\frac{(n')^{i'}}{L'}$$

(Note that we have $L$ instead of $L + 1$ in the denominator because we have already conditioned on $x^Y \not \leftarrow X^*$.) If we were to write this as a matrix, the probability $P(X^i = X^{i'}| x^Y, x^{Y'} \not \leftarrow X^*)$ is just the sum over all ways $X^i$ could equal $X^{i'}$, which is the sum of the diagonal. Therefore

Therefore

$$P(X^i = X^{i'}| x^Y, x^{Y'} \not \leftarrow X^*) = \sum\limits_{l = 1}^M\frac{n^l(n')^l}{LL'} = \frac{\mathbf{n}^T\mathbf{n}'}{LL'}.$$

Thus 

$$P(x^Y = x^{Y'}| x^Y, x^{Y'} \not \leftarrow X^*) = \frac{\mathbf{n}^T\mathbf{n}'}{LL'} + \left(1 - \frac{\mathbf{n}^T\mathbf{n}'}{LL'}\right)\frac{1}{Z}$$

so 
$$P(\textrm{Case 4}) = \frac{L}{L+1}\frac{L'}{L'+1}\left[\frac{\mathbf{n}^T\mathbf{n}'}{LL'} + \left(1 - \frac{\mathbf{n}^T\mathbf{n}'}{LL'}\right)\frac{1}{Z}\right].$$

The total probability $P(x^Y = x^Y')$ is the sum $P(\textrm{Case 1}) + P(\textrm{Case 2}) + P(\textrm{Case 3}) + P(\textrm{Case 4})$ is then

$$P(x^Y = x^Y') = \frac{1}{L+1}\frac{1}{L'+1} + \frac{1}{L+1}\frac{L'}{L'+1}\frac{1}{Z} + \frac{1}{L'+1}\frac{L}{L+1}\frac{1}{Z}$$

$$ + \frac{L}{L+1}\frac{L'}{L'+1}\left[\frac{\mathbf{n}^T\mathbf{n}'}{LL'} + \left(1 - \frac{\mathbf{n}^T\mathbf{n}'}{LL'}\right)\frac{1}{Z}\right]$$

$$= \frac{1}{(L+1)(L'+1)}\left[1 + \frac{L + L'}{Z} + \mathbf{n}^T\mathbf{n}' + \left(1 - \frac{\mathbf{n}^T\mathbf{n}'}{LL'}\right)\frac{LL'}{Z}\right]$$

$$= \frac{1}{(L+1)(L'+1)}\left[1 + \frac{L + L' + LL'}{Z} + \frac{Z-1}{Z}\mathbf{n}^T\mathbf{n}'\right]$$

which tends to 

$$\frac{1 + \mathbf{n}^T\mathbf{n}'}{(L+1)(L'+1)}$$

for large $Z \gg LL'$, which collapses to $1/[(L+1)(L'+1)]$ when $Y$ and $Y'$ share no symbols ($\mathbf{n}^T\mathbf{n}' = 0$).

\subsection{Similarity between codes for arbitrary sequences}

Let $Y = (y_1, ..., y_L)$ and $Y' = (y'_1, .., y'_{L'})$ be two sequences. Suppose they share a starting subsequence of exactly length $t$, i.e. $y_1 = y'_1, ..., y_t = y'_t$, but either $y_{t+1} \neq y'_{t+1}$, $t = L$, or $t = L'$. We write this condition as $Y_{1:t} \neq Y'_{1:t}$. Similarly, we refer to the second halves of the sequences (the subsequences in which they differ) as $Y_{t+1:L}$ and $Y'_{t+1:L'}$, respectively. (Starting subsequences are special because sequences are reconstructed from beginning to end.)

Then we have:

Note that when $Y$ and $Y'$ do not start with the same subsequence (i.e. $t = 0$) this reduces to

$$P(x^Y = x^{Y'}) = \frac{1}{(L+1)(L'+1)}\left[1 + \frac{L + L' + LL'}{Z} + \frac{Z-1}{Z}\mathbf{n}^T\mathbf{n}'\right]$$

\textbf{Proof:}

Since $Y_{1:t} = Y'_{1:t}$,  first note that $X^{Y_{1:t}} = X^{Y'_{1:t}}$, so $x^{Y_{1:t}} = x^{Y'_{1:t}}$. We wish to find $P(x^Y = x^{Y'})$.

The event $x^Y = x^{Y'}$ can occur in 4 distinct ways:

\textbf{Case 1}: $(x^Y \leftarrow X^{Y_{1:t}}) and (x^{Y'} \leftarrow X^{Y'_{1:t}})$, i.e. neither $x$ is overwritten from $t+1$ to $L$ or $L'$. These events are independent, so

$$P(\textrm{Case 1}) = P(x^Y \leftarrow X^{Y_{1:t}})P(x^{Y'} \leftarrow X^{Y'_{1:t}}) = \frac{t+1}{L+1}\frac{t+1}{L'+1}$$

\textbf{Case 2}: $(x^Y \not\leftarrow Y_{1:t}) and (x^{Y'} \not\leftarrow Y'_{1:t}) and (x^Y = x^{Y'})$, i.e. $x$ is overwritten in both sequences at some point between $t+1$ and $L$ or $L'$, but by the same c-state. Since the first two conditions are again independent, 

$$P(\textrm{Case 2}) = \frac{L-t}{L+1}\frac{L'-t}{L'+1}P(x^Y = x^{Y'}|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}})$$

where we recall that $X^{Y_{1:t}} = X^{Y'_{1:t}}$. If both $x$'s are overwritten, then they can end up with the same c-state if (1) $x^Y$ and $x^{Y'}$ are taken from the same symbol code $X^i$ (for any symbol code), which probability we write as $P(x^Y, x^{Y'} \leftarrow same|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}})$ or (2) $x^Y$ and $x^{Y'}$ are taken from different symbol codes but end up with the same c-state, which probability is given by $P(x^Y, x^{Y'} \not\leftarrow same|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}})(1/Z)$. That is,

$$P(x^Y = x^{Y'}|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}}) =$$

$$P(x^Y, x^{Y'} \leftarrow same|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}}) + P(x^Y, x^{Y'} \not\leftarrow same|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}})\frac{1}{Z}.$$

$P(x^Y, x^{Y'} \leftarrow same|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}})$ is the sum over symbol codes $X^i$ of the probabilities that both $x^Y$ and $x^{Y'}$ ended up being taken from $X^i$. These events are independent and equal to $n^i_{t+1:L}/(L-t)$ and $(n'_{t+1:L'})^i/(L'-t)$, respectively, so

$$P(x^Y, x^{Y'} \leftarrow same|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}}) = \sum_i \frac{n^i_{t+1:L}(n'_{t+1:L'})^i}{(L-t)(L'-t)} = \frac{\mathbf{n}^T_{t+1:L}\mathbf{n}'_{t+1:L'}}{(L-t)(L'-t)}$$

Thus

$$P(x^Y = x^{Y'}|x^Y, x^{Y'} \not\leftarrow X^{Y_{1:t}}) = \frac{\mathbf{n}^T_{t+1:L}\mathbf{n}'_{t+1:L'}}{(L-t)(L'-t)} + \left(1 - \frac{\mathbf{n}^T_{t+1:L}\mathbf{n}'_{t+1:L'}}{(L-t)(L'-t)} \right)\frac{1}{Z}$$

so

$$P(\textrm{Case 2}) = \frac{L-t}{L+1}\frac{L'-t}{L'+1}\left[\frac{\mathbf{n}^T_{t+1:L}\mathbf{n}'_{t+1:L'}}{(L-t)(L'-t)} + \left(1 - \frac{\mathbf{n}^T_{t+1:L}\mathbf{n}'_{t+1:L'}}{(L-t)(L'-t)} \right)\frac{1}{Z}\right].$$

\textbf{Case 3}: $(x^Y \leftarrow X^{Y_{1:t}}) and (x^{Y'} \not\leftarrow X^{Y'_{1:t}}) and (x^Y = x^{Y'})$, i.e. $x$ is untouched in the first sequence but overwritten in the second, but happens to be overwritten by $x^{Y_{1:t}}$. Once again the first two events are independent, so 

$$P(\textrm{Case 3}) = \frac{t+1}{L+1}\frac{L'-t}{L'+1}P(x^Y = x^{Y'}|x^Y \leftarrow X^{Y_{1:t}}, x^{Y'} \not\leftarrow X^{Y'_{1:t}}).$$

By similar reasoning as in Case 2, the event $(x^Y = x^{Y'}|x^Y \leftarrow X^{Y_{1:t}}, x^{Y'} \not\leftarrow X^{Y'_{1:t}})$ can occur if (1) $x^Y$ and $x^{Y'}$ are taken from the same symbol code $X^i$ (for any symbol code) or (2) $x^Y$ and $x^{Y'}$ are taken from different symbol codes but end up with the same c-state, whose probability is $P(x^Y, x^{Y'} \not\leftarrow same|x^Y \leftarrow X^{Y_{1:t}}, x^{Y'} \not\leftarrow X^{Y'_{1:t}})(1/Z)$.

As in Case 2 we compute $P(x^Y, x^{Y'} \leftarrow same|x^Y \leftarrow X^{Y_{1:t}}, x^{Y'} \not\leftarrow X^{Y'_{1:t}})$ by computing the sum over all $X^i$ of the probabilities that both $x^Y$ and $x^{Y'}$ were taken from $X^i$. Luckily, which symbol $x^Y$ was taken from in $Y_{1:t}$ and which symbol $x^{Y'}$ was taken from in $Y'_{t+1:L'}$ remain independent, so we have

$$P(x^Y, x^{Y'} \leftarrow same|x^Y \leftarrow X^{Y_{1:t}}, x^{Y'} \not\leftarrow X^{Y'_{1:t}}) = $$

$$\sum_i P(x^Y \leftarrow X^i|x^Y \leftarrow X^{Y_{1:t}}, x^{Y'} \not\leftarrow X^{Y'_{1:t}})P(x^{Y'} \leftarrow X^i|x^Y \leftarrow X^{Y_{1:t}}, x^{Y'} \not\leftarrow X^{Y'_{1:t}})$$

$$= \sum_i \frac{n^i_{1:t}}{t+1} \frac{(n'_{t+1:L'})^i}{L'-t} = \frac{\mathbf{n}^T_{1:t}\mathbf{n}'_{t+1:L'}}{(t+1)(L'-t)}$$

Thus

$$P(x^Y = x^{Y'}|x^Y \leftarrow X^{Y_{1:t}}, x^{Y'} \not\leftarrow X^{Y'_{1:t}}) = \frac{\mathbf{n}^T_{1:t}\mathbf{n}'_{t+1:L'}}{(t+1)(L'-t)} + \left(1 - \frac{\mathbf{n}^T_{1:t}\mathbf{n}'_{t+1:L'}}{(t+1)(L'-t)}\right)\frac{1}{Z}$$

so 

$$P(\textrm{Case 3}) = \frac{t+1}{L+1}\frac{L'-t}{L'+1}\left[\frac{\mathbf{n}^T_{1:t}\mathbf{n}'_{t+1:L'}}{(t+1)(L'-t)} + \left(1 - \frac{\mathbf{n}^T_{1:t}\mathbf{n}'_{t+1:L'}}{(t+1)(L'-t)}\right)\frac{1}{Z}\right].$$

\textbf{Case 4}: $(x^{Y'} \leftarrow X^{Y'_{1:t}}) and (x^Y \not\leftarrow X^{Y_{1:t}}) and (x^Y = x^{Y'})$, i.e. $x$ is untouched in the second sequence but overwritten in the first, but happens to be overwritten by $x^{Y'_{1:t}}$. This is symmetric to Case 3, so 

$$P(\textrm{Case 4}) = \frac{t+1}{L'+1}\frac{L-t}{L+1}\left[\frac{\mathbf{n}'^T_{1:t}\mathbf{n}_{t+1:L}}{(t+1)(L-t)} + \left(1 - \frac{\mathbf{n}'^T_{1:t}\mathbf{n}_{t+1:L}}{(t+1)(L-t)}\right)\frac{1}{Z}\right].$$

\textbf{Sum of all cases}: The total probability $P(x^Y = x^{Y'}) = P(\textrm{Case 1}) + P(\textrm{Case 2}) + P(\textrm{Case 3}) + P(\textrm{Case 4})$ is then

$$P(x^Y = x^{Y'}) = \frac{t+1}{L+1}\frac{t+1}{L'+1} + $$

$$\frac{L-t}{L+1}\frac{L'-t}{L'+1}\left[\frac{\mathbf{n}^T_{t+1:L}\mathbf{n}'_{t+1:L'}}{(L-t)(L'-t)} + \left(1 - \frac{\mathbf{n}^T_{t+1:L}\mathbf{n}'_{t+1:L'}}{(L-t)(L'-t)} \right)\frac{1}{Z}\right] + $$

$$\frac{t+1}{L+1}\frac{L'-t}{L'+1}\left[\frac{\mathbf{n}^T_{1:t}\mathbf{n}'_{t+1:L'}}{(t+1)(L'-t)} + \left(1 - \frac{\mathbf{n}^T_{1:t}\mathbf{n}'_{t+1:L'}}{(t+1)(L'-t)}\right)\frac{1}{Z}\right] + $$

$$\frac{t+1}{L'+1}\frac{L-t}{L+1}\left[\frac{\mathbf{n}'^T_{1:t}\mathbf{n}_{t+1:L}}{(t+1)(L-t)} + \left(1 - \frac{\mathbf{n}'^T_{1:t}\mathbf{n}_{t+1:L}}{(t+1)(L-t)}\right)\frac{1}{Z}\right]$$


$$
=\frac{1}{(L+1)(L'+1)}\left[(t+1)^2 + \frac{Z-1}{Z}\left[\mathbf{n}^T_{t+1:L}\mathbf{n}'_{t+1:L'} + \mathbf{n}^T_{1:t}\mathbf{n}'_{t+1:L'} + \mathbf{n}'^T_{1:t}\mathbf{n}_{t+1:L}\right]\right]
$$

$$
+ \frac{1}{(L+1)(L'+1)}\frac{1}{Z}\left[(L-t)(L'-t) + (t+1)(L'-t) + (t+1)(L-t)\right]
$$

This means that when $Y'$ is a starting subsequence of $Y$, i.e. $Y = (y'_1, ..., y'_t, y_{t+1}, ..., y_L)$ then the two sequences' expected similarity is equal to

$$P(x^{Y'} = x^Y) = \frac{1}{(L+1)(t+1)}\left((t+1)^2 + \frac{Z-1}{Z}\mathbf{n}^T_{1:t}\mathbf{n}_{t+1:L} + \frac{(t+1)(L-t)}{Z}\right).$$

\section{Adding composite symbols to the dictionary}

\section{Comparison to existing hyperdimensional computing codes}




\bibliographystyle{unsrt}  
%\bibliography{references}  %%% Remove comment to use the external .bib file (using bibtex).
%%% and comment out the ``thebibliography'' section.


%%% Comment out this section when you \bibliography{references} is enabled.
\begin{thebibliography}{1}

\bibitem{kour2014real}
George Kour and Raid Saabne.
\newblock Real-time segmentation of on-line handwritten arabic script.
\newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  International Conference on}, pages 417--422. IEEE, 2014.

\bibitem{kour2014fast}
George Kour and Raid Saabne.
\newblock Fast classification of handwritten on-line arabic characters.
\newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  International Conference of}, pages 312--318. IEEE, 2014.

\bibitem{hadash2018estimate}
Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  Jacovi.
\newblock Estimate and replace: A novel approach to integrating deep neural
  networks with existing applications.
\newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

\end{thebibliography}


\end{document}
